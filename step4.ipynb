{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Initializing a Connection to Spark\n",
    "\n",
    "We'll open a connection to Spark as follows. Note that Spark has multiple interfaces, as you will see if you look at sample code elsewhere. `SparkSession` is the “most modern” one and we’ll be using it for this course.  From `SparkSession`, you can load data into Spark DataFrames as well as `RDD`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to run from an environment outside of the Docker container you'll need to uncomment \n",
    "# and run this.  Otherwise you can skip through.\n",
    "# ! pip install pyspark --user\n",
    "# ! pip install seaborn --user\n",
    "# ! pip install plotly --user\n",
    "# ! pip install imageio --user\n",
    "# ! pip install folium --user\n",
    "# ! pip install heapq\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.gridspec as gridspec \n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "# graph viz\n",
    "import plotly.offline as pyo\n",
    "from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#map section\n",
    "import imageio\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "#graph section\n",
    "import networkx as nx\n",
    "import heapq  # for getting top n number of things from list,dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.106:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fed21a3fe10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql as sql\n",
    "\n",
    "import os\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/ipython3'\n",
    "\n",
    "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Download data\n",
    "\n",
    "The following code retrieves the Yelp dataset in a zipfile and decompresses it.  It will take quite a while - you may want to take a break while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "\n",
    "link:https://drive.google.com/open?id=19C4_AQ8_hUOgxVcQznAYDoW3tiPZlE1f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 Load Our Graph Datasets.\n",
    "\n",
    "For this assignment, we’ll be looking at graph data (reviews, reviewers, businesses) downloaded from Yelp.\n",
    "\n",
    "**A very brief review of graph theory**. Recall that a graph $G$ is composed of a set of vertices $V$ (also called nodes) and edges $E$ (sometimes called links).  Each vertex $v \\in V$ has an identity (often represented in the real world as a string or numeric “node ID”).  Each edge $e \\in E$ is a tuple $(v_i,...,v_j)$ where $v_i$ represents the source or origin of the edge, and $v_j$ represents the target or destination.  In the simplest case, the edge tuple above is simply the pair $(v_i,v_j)$ but in many cases we may have additional fields such as a label or a distance.  Recall also that graphs may be undirected or directed; in undirected graphs, all edges are symmetric whereas in directed graphs, they are not.  For instance, airline flights are directed, whereas Facebook friend relationships are undirected.\n",
    "\n",
    "Let’s read our social graph data from Yelp, which forms a directed graph.  Here, the set of nodes is also not specified; the assumption is that the only nodes that matter are linked to other nodes, and thus their IDs will appear in the set of edges.  To load the file `input.txt` into a Spark DataFrame, you can use lines like the following.\n",
    "\n",
    "```\n",
    "# Read lines from the text file\n",
    "input_sdf = spark.read.load('input.txt', format=\"text\")\n",
    "```\n",
    "\n",
    "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” much as we used `_df` to denote a Pandas DataFrame in Homework 1.  Load the various files from Yelp.\n",
    "\n",
    "Your datasets should be named `yelp_business_sdf`, `yelp_business_attributes_sdf`, `yelp_business_horus_sdf`, `yelp_check_in_sdf`, `yelp_reviews_sdf`, and `yelp_users_sdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: load Yelp datasets\n",
    "\n",
    "# Worth 1 point per successful load, 5 additional points if valid schema\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# Read yelp Business file\n",
    "yelp_business_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"file:///home/ashrumochan/yelp_data/yelp_business.csv\")\n",
    "#Read yelp Business hours data from csv file\n",
    "yelp_business_hours_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "                        .load(\"file:///home/ashrumochan/yelp_data/yelp_business_hours.csv\")\n",
    "    \n",
    "yelp_reviews_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    " .load(\"file:///home/ashrumochan/yelp_data/yelp_review2.csv\")\n",
    "\n",
    "yelp_business_attributes_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    ".load(\"file:///home/ashrumochan/yelp_data/yelp_business_attributes.csv\")\n",
    "\n",
    "yelp_check_in_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    ".load(\"file:///home/ashrumochan/yelp_data/yelp_checkin.csv\")\n",
    "\n",
    "yelp_users_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"file:///home/ashrumochan/yelp_data/yelp_user.csv\")\n",
    "    \n",
    "yelp_tip_sdf=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"file:///home/ashrumochan/yelp_data/yelp_tip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+\n",
      "|         business_id|                name|neighborhood|             address|          city|state|postal_code|     latitude|     longitude|stars|review_count|is_open|          categories|\n",
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+\n",
      "|FYWN1wneV18bWNgQj...|    Dental by Design|        null|4855 E Warner Rd,...|     Ahwatukee|   AZ|      85044|   33.3306902|  -111.9785992|  4.0|          22|      1|Dentists;General ...|\n",
      "|He-G7vWjzVUysIKrf...| Stephen Szabo Salon|        null|  3101 Washington Rd|      McMurray|   PA|      15317|   40.2916853|   -80.1048999|  3.0|          11|      1|Hair Stylists;Hai...|\n",
      "|KQPW8lFf1y5BT2Mxi...|Western Motor Veh...|        null|6025 N 27th Ave, ...|       Phoenix|   AZ|      85017|   33.5249025|  -112.1153098|  1.5|          18|      1|Departments of Mo...|\n",
      "|8DShNS-LuFqpEWIp0...|    Sports Authority|        null|5000 Arizona Mill...|         Tempe|   AZ|      85282|   33.3831468|  -111.9647254|  3.0|           9|      0|Sporting Goods;Sh...|\n",
      "|PfOCPjBrlQAnz__NX...|Brick House Taver...|        null|        581 Howe Ave|Cuyahoga Falls|   OH|      44221|   41.1195346|   -81.4756898|  3.5|         116|      1|American (New);Ni...|\n",
      "|o9eMRCWt5PkpLDE0g...|             Messina|        null|      Richterstr. 11|     Stuttgart|   BW|      70567|      48.7272|       9.14795|  4.0|           5|      1| Italian;Restaurants|\n",
      "|kCoE3jvEtg6UVz5SO...|          BDJ Realty|   Summerlin|2620 Regatta Dr, ...|     Las Vegas|   NV|      89128|     36.20743|    -115.26846|  4.0|           5|      1|Real Estate Servi...|\n",
      "|OD2hnuuTJI9uotcKy...|         Soccer Zone|        null|7240 W Lake Mead ...|     Las Vegas|   NV|      89128|   36.1974844|  -115.2496601|  1.5|           9|      1|Shopping;Sporting...|\n",
      "|EsMcGiZaQuG1OOvL9...|    Any Given Sundae|        null|2612 Brandt Schoo...|       Wexford|   PA|      15090|40.6151022445|-80.0913487465|  5.0|          15|      1|Coffee & Tea;Ice ...|\n",
      "|TGWhGNusxyMaA4kQV...|Detailing Gone Mo...|        null|                null|     Henderson|   NV|      89014|36.0558252127| -115.04635039|  5.0|           7|      1|Automotive;Auto D...|\n",
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_business_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_hours_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_attributes_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_check_in_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_users_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('review_id', 'string'),\n",
       " ('user_id', 'string'),\n",
       " ('business_id', 'string'),\n",
       " ('stars', 'string'),\n",
       " ('date', 'string'),\n",
       " ('text', 'string'),\n",
       " ('useful', 'string'),\n",
       " ('funny', 'string'),\n",
       " ('cool', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if yelp_reviews_sdf.dtypes[0][1] != 'string':\n",
    "    raise ValueError('Unexpected datatype on ' + yelp_reviews.dtypes[0][0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4 Simple Wrangling in Spark DataFrames\n",
    "\n",
    "Currently, some of the data from the Yelp dataset is a bit ugly.\n",
    "\n",
    "You should:\n",
    "\n",
    "* Clean `yelp_business_hours` so `\"None\"` is replaced by a Spark `null`.\n",
    "* Clean `yelp_users` so `\"None\"` is replaced by a Spark `null`.\n",
    "\n",
    "\n",
    "\n",
    "First, create SQL tables for each of your Spark DataFrames.  Take the same names as you've used previously, except remove the `_sdf` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save tables with names such as yelp_business, yelp_users\n",
    "\n",
    "# Worth 5 points if done successfully\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "yelp_business_hours_sdf.registerTempTable(\"yelp_business_hours\")\n",
    "yelp_business_attributes_sdf.registerTempTable(\"yelp_business_attributes\")\n",
    "yelp_business_sdf.registerTempTable(\"yelp_business\")\n",
    "yelp_check_in_sdf.registerTempTable(\"yelp_check_in\")\n",
    "yelp_reviews_sdf.registerTempTable(\"yelp_reviews\")\n",
    "yelp_tip_sdf.registerTempTable(\"yelp_tip\")\n",
    "yelp_users_sdf.registerTempTable(\"yelp_users_sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark.sql('SELECT COUNT(*) AS count FROM yelp_business').take(1)[0]['count'] != 174567:\n",
    "    raise ValueError(\"Unexpected count or table not found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll need to define a **user-defined function** `replace_none_with_null` to apply to your string fields.  It should take a string parameter and compare it with `None` and `Na`.  If there is a match to either, it should return the Python None value (which will become a Spark null), otherwise it shoudl return the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function here.\n",
    "# Worth 7 points if done successfully\n",
    "\n",
    "def replace_none_with_null(x):\n",
    "    if x==\"None\" or x==\"Na\":\n",
    "        return ''\n",
    "    else:\n",
    "        return x\n",
    "def replace_none_with_nulls(x):\n",
    "    return F.when(F.col(x) != \"None\", F.col(x)).otherwise(\"Null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_none_with_null('None'):\n",
    "    raise ValueError('Your function does not work')\n",
    "    \n",
    "if not replace_none_with_null('x'):\n",
    "    raise ValueError('Your function does not work')     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap the Python code in a Spark UDF\n",
    "\n",
    "# We're providing this since it's basically a template\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types\n",
    "\n",
    "spark.udf.register(\"replace_none_with_null\", replace_none_with_null)\n",
    "\n",
    "spark_replace_none_with_null = udf(replace_none_with_null, pyspark.sql.types.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `replace_with_none` in SQL, or `spark_replace_none_with_null` if you prefer Pandas-style Spark statements, to clean the data as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean yelp_business_hours_sdf\n",
    "\n",
    "# Worth 5 points for each of the two tables specified in 2.4 instructions, if done successfully\n",
    "\n",
    "# YOUR CODE HERE\n",
    "spark_replace_none_with_null = F.udf(replace_none_with_null, sql.types.StringType())\n",
    "yelp_business_hours_sdf1 = yelp_business_hours_sdf.rdd.map(lambda x: replace_none_with_null(x))\n",
    "yelp_business_hours_sdf=yelp_business_hours_sdf1.toDF()\n",
    "columns = yelp_business_hours_sdf.columns\n",
    "for col1 in columns:\n",
    "    yelp_business_hours_sdf = yelp_business_hours_sdf.withColumn(col1, replace_none_with_nulls(col1))\n",
    "    \n",
    "yelp_business_hours_sdf.registerTempTable(\"yelp_business_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+----------+----------+---------+\n",
      "|         business_id|    monday|   tuesday| wednesday|  thursday|    friday|  saturday|   sunday|\n",
      "+--------------------+----------+----------+----------+----------+----------+----------+---------+\n",
      "|FYWN1wneV18bWNgQj...| 7:30-17:0| 7:30-17:0| 7:30-17:0| 7:30-17:0| 7:30-17:0|      Null|     Null|\n",
      "|He-G7vWjzVUysIKrf...|  9:0-20:0|  9:0-20:0|  9:0-20:0|  9:0-20:0|  9:0-16:0|  8:0-16:0|     Null|\n",
      "|KQPW8lFf1y5BT2Mxi...|      Null|      Null|      Null|      Null|      Null|      Null|     Null|\n",
      "|8DShNS-LuFqpEWIp0...| 10:0-21:0| 10:0-21:0| 10:0-21:0| 10:0-21:0| 10:0-21:0| 10:0-21:0|11:0-19:0|\n",
      "|PfOCPjBrlQAnz__NX...|  11:0-1:0|  11:0-1:0|  11:0-1:0|  11:0-1:0|  11:0-1:0|  11:0-2:0| 11:0-0:0|\n",
      "|o9eMRCWt5PkpLDE0g...|  18:0-0:0|  18:0-0:0|  18:0-0:0|  18:0-0:0|  18:0-0:0|  18:0-0:0|     Null|\n",
      "|kCoE3jvEtg6UVz5SO...|  8:0-17:0|  8:0-17:0|  8:0-17:0|  8:0-17:0|  8:0-17:0|      Null|     Null|\n",
      "|OD2hnuuTJI9uotcKy...| 11:0-19:0| 11:0-19:0| 11:0-19:0| 11:0-19:0| 11:0-19:0| 10:0-18:0|11:0-16:0|\n",
      "|EsMcGiZaQuG1OOvL9...|      Null|      Null|      Null|      Null|      Null|      Null|     Null|\n",
      "|TGWhGNusxyMaA4kQV...|  9:0-18:0|  9:0-18:0|  9:0-18:0|  9:0-18:0|  9:0-18:0|  9:0-18:0| 9:0-18:0|\n",
      "|XOSRcvtaKc_Q5H1SA...|      Null|      Null|      Null|      Null|      Null|      Null|     Null|\n",
      "|Y0eMNa5C-YU1RQOZf...| 9:30-18:0| 9:30-18:0| 9:30-18:0| 9:30-18:0| 9:30-18:0|8:30-17:30|11:0-15:0|\n",
      "|xcgFnd-MwkZeO5G2H...|      Null|      Null|      Null|      Null|      Null|      Null|     Null|\n",
      "|NmZtoE3v8RdSJEczY...|  8:0-17:0|  8:0-17:0|  8:0-14:0|  8:0-17:0|  8:0-17:0|      Null|     Null|\n",
      "|fNMVV_ZX7CJSDWQGd...|  7:0-15:0|  7:0-15:0|  7:0-15:0|  7:0-15:0|  7:0-15:0|      Null|     Null|\n",
      "|l09JfMeQ6ynYs5MCJ...|  9:0-22:0|  9:0-22:0|  9:0-22:0|  9:0-22:0|  9:0-22:0|  9:0-22:0| 9:0-22:0|\n",
      "|IQSlT5jGE6CCDhSG0...|  9:0-19:0|  9:0-19:0|  9:0-19:0|  9:0-19:0|  9:0-19:0|  9:0-18:0|10:0-17:0|\n",
      "|b2I2DXtZVnpUMCXp1...| 7:30-18:0| 7:30-18:0| 7:30-18:0| 7:30-18:0| 7:30-18:0| 7:30-16:0|     Null|\n",
      "|0FMKDOU8TJT1x87OK...|  9:0-18:0|  9:0-18:0|  9:0-18:0|  9:0-18:0|  9:0-19:0|  9:0-18:0|     Null|\n",
      "|Gu-xs3NIQTj3Mj2xY...|11:30-22:0|11:30-22:0|11:30-22:0|11:30-22:0|11:30-23:0| 11:0-23:0|11:0-22:0|\n",
      "+--------------------+----------+----------+----------+----------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_business_hours_sdf.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark.sql('select count(*) as count from yelp_business_hours where wednesday=\\'None\\'').take(1)[0]['count'] > 0:\n",
    "    raise ValueError('Did not successfully clean business hours')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean yelp_users, which has schema\n",
    "# |user_id|   name|review_count|yelping_since|             friends|useful|funny|cool|fans|elite|average_stars|compliment_hot\n",
    "# |compliment_more|compliment_profile|compliment_cute|compliment_list|compliment_note|compliment_plain|compliment_cool\n",
    "# |compliment_funny|compliment_writer|compliment_photos|\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "columns2=yelp_users_sdf.columns\n",
    "columns2\n",
    "for col2 in columns2:\n",
    "    yelp_users_sdf = yelp_users_sdf.withColumn(col2, replace_none_with_nulls(col2))\n",
    "    \n",
    "yelp_users_sdf.registerTempTable(\"yelp_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark.sql('select count(*) as count from yelp_users where elite=\\'None\\'').take(1)[0]['count'] > 0:\n",
    "    raise ValueError('Did not successfully clean users')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5 Simple Analytics on the Data\n",
    "\n",
    "In this section, we shall be executing Spark operations on the data given. Beyond simply executing the queries, you may try using `.explain()` method to see more about the query execution. Also, please read the data description prior to attempting the following questions to understand the data.\n",
    "\n",
    "#### 2.5.1 Businesses with the best average review\n",
    "\n",
    "Compute, stored in `best_average_sdf`, the list of names of businesses based on their average review score (review stars), in decreasing order, sorted lexicographically (in increasing order) by name if they have the same score.  Output the number of reviews also.  Call the columns `name`, `avg_rating`, and `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['review_id',\n",
       "  'user_id',\n",
       "  'business_id',\n",
       "  'stars',\n",
       "  'date',\n",
       "  'text',\n",
       "  'useful',\n",
       "  'funny',\n",
       "  'cool'],\n",
       " DataFrame[business_id: string, name: string, neighborhood: string, address: string, city: string, state: string, postal_code: string, latitude: double, longitude: double, stars: double, review_count: int, is_open: int, categories: string])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews_sdf.columns, yelp_business_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Businesses with best average review\n",
    "\n",
    "# Worth 5 points if done successfully\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "best_average_sdf=spark.sql(\"select u.name, round(avg(r.stars)) as avg_Rating ,count(review_id) as count \\\n",
    "                                from yelp_reviews r join yelp_business u\\\n",
    "                           on r.business_id=u.business_id\\\n",
    "                           group by u.name order by avg_Rating desc\")\n",
    "# sqlContext.sql(\"\"\"SELECT DISTINCT customer.name AS name FROM purchase JOIN book ON purchase.isbn = book.isbn \\\n",
    "# JOIN customer ON customer.cid = purchase.cid WHERE customer.name != \\\n",
    "# 'Harry Smith' AND purchase.isbn IN (SELECT purchase.\\\n",
    "# isbn FROM customer JOIN purchase ON customer.cid = purchase.cid WHERE customer.name = 'Harry Smith')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = best_average_sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_review=spark.sql(\"select round(avg(stars))  avg_review from yelp_reviews\")\n",
    "\n",
    "\n",
    "avg_list=[(i.avg_review) for i in avg_review.select('avg_review').collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Users who are more negative than average\n",
    "\n",
    "Find the users whose average review is below the *average of the per-user* average reviews.  Think about how to factor that into steps!\n",
    "\n",
    "* Compute the (floating-point) variable `overall_avg` as the average of the users' average reviews. (You might compute this in a Spark DataFrame first).\n",
    "* Then output `negative_users_sdf` as the users whose average rating is below that.  This Spark dataframe should have `name` and `avg_rating` and should be sorted first (from lowest to highest) by average rating, then lexicographically (in ascending order) by name.  You should drop cases where the name is null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute overall_avg as a VALUE, and negative_users_sdf as a Spark dataframe\n",
    "# Worth 5 points each\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError(),\n",
    "avg_review=spark.sql(\"select round(avg(stars))  avg_review from yelp_reviews\")\n",
    "avg_list=[(i.avg_review) for i in avg_review.select('avg_review').collect()]\n",
    "overall_avg=\" \".join(str(x) for x in avg_list)\n",
    "a=44\n",
    "negative_users_sdf=spark.sql(\"select u.name, r.stars as below_avg_rating  from yelp_users u join yelp_reviews r \\\n",
    "                              on u.user_id=r.user_id where r.stars < {}  \\\n",
    "                              ORDER BY r.stars asc \".format(overall_avg) ).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not overall_avg:\n",
    "    raise ValueError('Forgot to compute the overall average')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|       name|below_avg_rating|\n",
      "+-----------+----------------+\n",
      "|      Jason|               1|\n",
      "|      Luann|               1|\n",
      "|Christopher|               1|\n",
      "|       Russ|               1|\n",
      "|   Brittany|               1|\n",
      "|       Zach|               1|\n",
      "|         ab|               1|\n",
      "|      Scott|               1|\n",
      "|         ab|               1|\n",
      "|     Ashley|               1|\n",
      "+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_users_sdf.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.4 Cities by number of businesses\n",
    "\n",
    "Find the top 10 cities by number of (Yelp-listed) businesses.\n",
    "\n",
    "This time, use the `take()` function to create a *list* of the top 10 cities (as Rows).  Call this list `top10_cities` and make sure it includes city `name` and `num_restaurants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cities by number of businesses\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "yelp_business_sdf.columns\n",
    "\n",
    "#raise NotImplementedError()\n",
    "\n",
    "top10_cities= spark.sql(\"select u.city as City, count(*) as num_restaurants from yelp_business u\\\n",
    "                            GROUP BY city\").take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(top10_cities) != 10:\n",
    "    raise ValueError('Not top10!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Computing Simple Graph Centrality\n",
    "\n",
    "The study of networks has proposed a wide variety of measures for measuring importance of nodes.  A popular metric that is easy to compute is the degree centrality.  The degree centrality of a node is simply the number of connections to the node.  In a directed graph such as ours, you will want to compute both the indegree centrality (number of nodes with edges coming to this node) and outdegree centrality (number of nodes with edges coming from this node).\n",
    "\n",
    "## 3.1 Generate user-business graph\n",
    "\n",
    "For this step, we want to construct a *directed* graph with edges from users to business for every interaction (review, in our case). To do this, we will use the `yelp_reviews_sdf` dataframe and extract the `user_id` as `from_node` and `business_id` as `to_node` into a different dataframe called `review_graph_sdf`.  Finally, include the `stars` field but call it `score`.  Also make sure it is available as a table in SQL called `review_graph`.\n",
    "\n",
    "Some of the values may be null; remove these for `user_id` or `business_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create review graph SDF from yelp_reviews\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "review_graph_sdf=spark.sql(\"select user_id as from_node,business_id as to_node,\\\n",
    "                            stars as score from yelp_reviews\")\n",
    "review_graph_sdf=review_graph_sdf.dropna(subset=['from_node','to_node'])\n",
    "review_graph_sdf.registerTempTable(\"review_graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of number of edges\n",
    "review_graph_sdf.count()\n",
    "\n",
    "### BEGIN HIDDEN TEST\n",
    "if review_graph_sdf.count() != 5273700:\n",
    "    raise ValueError('Unexpected graph size')\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Businesses with highest indegree\n",
    "\n",
    "Find, in decreasing order, the businesses with the most (highest number of) reviews, using either `review_graph_sdf` (or its SQL version) or `yelp_reviews` as well as `yelp_business`.  The dataframe should have the fields `name`,`count`, and `rating` (average `score`).  Assign these to a new dataframe `most_reviewed_sdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: most_reviewed_sdf\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "most_reviewed_sdf=spark.sql(\"select u.name as Name, count(*) as count,\\\n",
    "                                round(avg(r.stars)) as Rating from yelp_business u join yelp_reviews r \\\n",
    "                                on u.business_id=r.business_id GROUP by Name\\\n",
    "                                ORDER BY count desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                Name|count|Rating|\n",
      "+--------------------+-----+------+\n",
      "|           Starbucks|19518|   3.0|\n",
      "|  Hash House A Go Go| 9840|   4.0|\n",
      "|          McDonald's| 9415|   2.0|\n",
      "|Chipotle Mexican ...| 7980|   3.0|\n",
      "|        Mon Ami Gabi| 7362|   4.0|\n",
      "|    Bacchanal Buffet| 7006|   4.0|\n",
      "|  Buffalo Wild Wings| 5998|   3.0|\n",
      "|        Wicked Spoon| 5951|   4.0|\n",
      "|     In-N-Out Burger| 5598|   4.0|\n",
      "| Gordon Ramsay BurGR| 5448|   4.0|\n",
      "+--------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_reviewed_sdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Outdegree\n",
    "\n",
    "Next get a list of users whose vertices have the highest outdegree, i.e., they write the most reviews.  Return this in a dataframe `prolific_reviewers_sdf` with fields `name` and `num_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reviews who created most reviews (beware duplicate names)\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#raise NotImplementedError()\n",
    "prolific_reviewers_sdf=spark.sql(\"Select u.name AS name, count(r.stars) as num_reviews from yelp_users u \\\n",
    "                                join yelp_reviews r on u.user_id=r.user_id\\\n",
    "                                 GROUP BY name order by num_reviews desc \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Michael', num_reviews=45190),\n",
       " Row(name='David', num_reviews=44056),\n",
       " Row(name='John', num_reviews=43566),\n",
       " Row(name='Jennifer', num_reviews=42405),\n",
       " Row(name='Chris', num_reviews=41125),\n",
       " Row(name='Mike', num_reviews=38512),\n",
       " Row(name='Jessica', num_reviews=31774),\n",
       " Row(name='Sarah', num_reviews=31733),\n",
       " Row(name='Michelle', num_reviews=29978),\n",
       " Row(name='Lisa', num_reviews=27100)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_reviewers = prolific_reviewers_sdf.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indegree also gives you the most reviewed restaurants and the outdegree gives you the information about the users who write the most reviews.\n",
    "\n",
    "For the advanced part of this Homework, we'll consider more complex measures than indegree / outdegree.  For now let's move on to the more general problem of graph traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. “Traversing” a Graph\n",
    "\n",
    "For our next tasks, we will be “walking” the graph and making connections.\n",
    "\n",
    "\n",
    "## 4.1 Distributed Breadth-First Search\n",
    "A search algorithm typically starts at a node or set of nodes, and “explores” or “walks” for some number of steps to find a match or a set of matches.\n",
    "\n",
    "Let’s implement a distributed version of a popular algorithm, breadth-first-search (BFS).  This algorithm is given a graph `G`, a set of origin nodes `N`, and a depth `d`.  In each iteration or round up to depth `d`, it explores the set of all new nodes directly connected to the nodes it already has seen, before going on to the nodes another “hop” away.  If we do this correctly, we will explore the graph in a way that (1) avoids getting caught in cycles or loops, and (2) visits each node in the fewest number of “hops” from the origin.  BFS is commonly used in tasks such as friend recommendation in social networks.\n",
    "\n",
    "**How does distributed BFS in Spark work**?  Let’s start with a brief sketch of standard BFS.  During exploration “rounds”, we can divide the graph into three categories:\n",
    "\n",
    "1. *unexplored nodes*.  These are nodes we have not yet visited.  You don’t necessarily need to track these separately from the graph.\n",
    "2. *visited nodes*.  We have already reached these nodes in a previous “round”.\n",
    "3. *frontier nodes*.  These are nodes we have visited in this round.  We have not yet checked whether they have out-edges connecting to unexplored nodes.\n",
    "\n",
    "We can illustrate these with a figure and an example.\n",
    "\n",
    "![Graph traversal](https://drive.google.com/uc?export=view&id=1I2Kc3uQcDlp7RsDqRQAfQAvS3F_VcJpA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the figure, which shows a digraph.  The green node A represents the origin.\n",
    "\n",
    "* In the first round, the origin A is the sole frontier node.  We find all nodes reachable directly from A, namely B-F; then we remove all nodes we have already visited (there are none) or that are in the frontier (the node A itself).  This leaves the blue nodes B-F, which are all reachable in (at most) 1 hop from A.\n",
    "* In the second round, we move A to the visited set and B-F to the frontier.  Now we explore all nodes connected directly to frontier nodes, namely A (from B), F (from E), and the red nodes G-L.  We eliminate the nodes already contained in the frontier and visited sets from the next round’s frontier set, leaving the red nodes only.\n",
    "* In the third round, we will move B-F to the visited set, G-L to the frontier set, and explore the next round of neighbors N-V.  This process continues up to some maximum depth (or until there are no more unexplored nodes).\n",
    "\n",
    "Assume we create data structures (we can make them DataFrames) for the visited and frontier nodes.  Consider (1) how to initialize the different sets at the start of computation (note: unexplored nodes are already in the graph), and (2) how to use the graph edges and the existing data structures to update state for the next iteration “round”.\n",
    "\n",
    "You might possibly have seen how to create a breadth-first-search algorithm in a single-CPU programming language, using a queue to capture the frontier nodes. With Spark we don’t need a queue -- we just need the three sets above.\n",
    "\n",
    "### 4.1.1 Breadth-First Search Algorithm\n",
    "\n",
    "Create a function `spark_bfs(G, origins, max_depth)` that takes a Spark DataFrame with a graph G (following the schema for `review_graph_sdf` described above, but to be treated as an **undirected graph**), a Python list-of-dictionaries `origins` of the form \n",
    "\n",
    "```\n",
    "[{‘node’: nid1}, \n",
    " {‘node’: nid2}, \n",
    " …]\n",
    "```\n",
    "\n",
    "and a nonnegative integer “exploration depth” `max_depth` (to only run BFS on a tractable portion of the graph).  The `max_depth` will be the maximum number of edge traversals (e.g., the origin is at `max_depth=0`, one hop from the origin is `max_depth=1`, etc.  The function should return a DataFrame containing pairs of the form (node, distance), where the distance is depth at which $n$ was *first* encountered (i.e., the shortest-path distance from the origin nodes).  Note that the origin nodes should also be returned in this Spark DataFrame (with depth 0)!  \n",
    "\n",
    "You can create a new Spark DataFrame with an integer `node` column from the above list of maps `origins`, as follows. This will give you a DataFrame of the nodes to start the BFS at\n",
    "\n",
    "```\n",
    "schema = StructType([\n",
    "            StructField(\"node\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    my_sdf = spark.createDataFrame(my_list_of_maps, schema)\n",
    "```\n",
    "\n",
    "In this algorithm, be careful in each iteration to keep only the nodes with their shortest distances (you may need to do aggregation or item removal).  You should accumulate all nodes at distances 0, 1, ..., `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: iterative search over undirected graph\n",
    "# Worth 5 points directly, but will be needed later\n",
    "\n",
    "def spark_bfs(G, origins, max_depth):\n",
    "    schema = StructType([\n",
    "                StructField(\"node\", StringType(), True),\n",
    "                StructField(\"distance\", IntegerType(), False)\n",
    "            ])\n",
    "    \n",
    "    return G\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_bfs(review_graph_sdf,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c91ddfad44ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'node'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bv2nCi5Qv5vroFiqKGopiw'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark_bfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_graph_sdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "orig = [{'node': 'bv2nCi5Qv5vroFiqKGopiw'}]\n",
    "\n",
    "count= spark_bfs(review_graph_sdf, orig, 3).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2. Restaurant Recommendation\n",
    "\n",
    "Now create a function `friend_rec` that takes in two arguments: the graph_sdf and the ID of a user, `me`.  Using `spark_bfs` it should recommend restaurants that are highly popular among the reviewers who reviewed the same restaurants as `me`.\n",
    "\n",
    "Then, take the review_graph, filter it to only consider 3-star reviews or above, and run `friend_rec` over the results and the user with ID `bv2nCi5Qv5vroFiqKGopiw`.  In the Spark dataframe `rec_rest_sdf`, give us the `name`s of the most-highly recommended restaurants, sorted primarily in descending order of count, and then secondarily by lexicographic order of name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: restaurant recommendation using spark_bfs\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_rest = rec_rest_sdf.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3. Friend Visualization\n",
    "\n",
    "#### 4.3.1: Loading data subsets\n",
    "A closer look at the `yelp_user` dataframe tells us that there is an attribute called `friends` that we can use in order to construct an undirected friend graph.  For this part of the assignment we'll go back to Pandas -- not Spark -- DataFrames.\n",
    "\n",
    "We will work with the first 200 entries from the `yelp_user` data file and visualize these users' friends.\n",
    "\n",
    "Read the first 200 entries of the `yelp_user.csv` file into a pandas dataframe called `user_200` (Remember: You can pass `nrows` as an option to the `pd.read_csv()`)\n",
    "\n",
    "We’ll subsequently make use of the `networkx` graph visualization tool, which lets us see what the graph actually looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read 200 entries\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3.2: Select users with at least one friend\n",
    "\n",
    "In this part select the friends from `user_200` where the value is not `None`. The `friends` column is a string with comma-separated `user_id`s as values. We will make use of `lambda` functions in order to extract the different `user_id` from this comma separated string.\n",
    "\n",
    "Select **only the users who have at least one friend**. That is, the `friends` column does not have the value \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find users with friends\n",
    "# Worth 2 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3.3: Extracting friend as list\n",
    "\n",
    "Pandas dataframe supports the use of `df.apply()` which can take a function as a parameter.\n",
    "\n",
    "Example use of `lambda` function with `.apply()`\n",
    "\n",
    "`df['col_2'] = df['col_1'].apply(lambda x: x+10)`\n",
    "\n",
    "The above statement will create a column 'col_2' in df with values of df['col_1'] + 10\n",
    "\n",
    "For the next step, make use of `lambda` functions to `split` the value of the `friends` column by `,` and apply this to create a new column called `list_friends`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: friend lists\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_200[['name','list_friends']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3.4: Obtaining Friend lists\n",
    "\n",
    "We are only interest in the `user_id` and `list_friends` columns.  Select these into a dataframe called `subset_users`.\n",
    "\n",
    "The `.stack()` option allows you to \"unnest\" the items in the list, associating them with the corresponding value of the index.  \n",
    "\n",
    "In our instance, we would like to `set_index()` on `user_id` so the user ID is the index.  Then if we `.apply()` the `stack()` operation on the `list_friends` column:\n",
    "\n",
    "```\n",
    "result_df = df.set_index(['col_1'])['col_2'].apply(pd.Series).stack()\n",
    "```\n",
    "\n",
    "We should get each element in the list associated with the appropriate `user_id`.  Performing `df.reset_index()` on `result_df` will give us `friend_data` in an edge table.  Rename the columns of `friend_data` to 'source', 'level_1', and 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: projection to users and lists of friends, stacked\n",
    "# Worth 3 points\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3.5: Visualization using Networkx\n",
    "\n",
    "In this step we will use the `networkx` library to visualize.\n",
    "\n",
    "\n",
    "You can create the graph from the networkx function `from_pandas_edgelist` and the `friend_data`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: networkx graph ready to draw\n",
    "# Worth 3 points\n",
    "\n",
    "import networkx as nx\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nx.draw(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
